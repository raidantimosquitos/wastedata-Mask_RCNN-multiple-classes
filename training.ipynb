{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFdhm8T12A0pZW9PT6rdv0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kD4KBGFw9piF"},"source":["# 1. Importing and cloning repositories\r\n","\r\n","First, we will clone a repository that contains some of the building code blocks for implementing Mask RCNN. The function copytree() will get the necessary files for you. After cloning the repository we will import the dataset. I am importing the dataset from my drive."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMXLmtT_aDie","executionInfo":{"status":"ok","timestamp":1615191953593,"user_tz":-60,"elapsed":131593,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}},"outputId":"1aa9b69f-f802-4543-cffa-77958b17dd54"},"source":["# Google Drive imports\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive',force_remount=True)\r\n","\r\n","!git clone \"https://github.com/raidantimosquitos/wastedata-Mask_RCNN-multiple-classes\"\r\n","import shutil, os"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","Cloning into 'wastedata-Mask_RCNN-multiple-classes'...\n","remote: Enumerating objects: 47, done.\u001b[K\n","remote: Counting objects: 100% (47/47), done.\u001b[K\n","remote: Compressing objects: 100% (38/38), done.\u001b[K\n","remote: Total 178 (delta 15), reused 24 (delta 6), pack-reused 131\u001b[K\n","Receiving objects: 100% (178/178), 12.35 MiB | 13.91 MiB/s, done.\n","Resolving deltas: 100% (16/16), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6JJ5PTMy-WvV","executionInfo":{"status":"ok","timestamp":1615191987865,"user_tz":-60,"elapsed":165856,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}},"outputId":"66849f79-3815-4991-84e9-c250ea6d6dea"},"source":["def copytree(src = '/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN', dst = '/content/', symlinks=False, ignore=None):\r\n","    try:\r\n","      shutil.rmtree('/content/.ipynb_checkpoints')\r\n","    except:\r\n","      pass\r\n","    for item in os.listdir(src):\r\n","      s = os.path.join(src, item)\r\n","      d = os.path.join(dst, item)\r\n","      if os.path.isdir(s):\r\n","        shutil.copytree(s, d, symlinks, ignore)\r\n","      else:\r\n","        shutil.copy2(s, d)\r\n","copytree()\r\n","shutil.copytree('/content/gdrive/MyDrive/mrcnn_fire/main/dataset','/content/dataset')"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/dataset'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Z5blFbLisKp","executionInfo":{"status":"ok","timestamp":1615191993635,"user_tz":-60,"elapsed":171619,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}},"outputId":"6be8e49c-8f7d-4fff-8160-ba1b90e6bc39"},"source":["!pip install keras==2.2.5\r\n","%tensorflow_version 1.x"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting keras==2.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n","\r\u001b[K     |█                               | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 23.8MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 10.5MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 163kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 174kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 184kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 194kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 204kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 215kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 225kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 235kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 245kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 256kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 266kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 276kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 286kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 296kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 307kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 317kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 327kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 8.7MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n","\u001b[?25hInstalling collected packages: keras-applications, keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.2.5 keras-applications-1.0.8\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kIfJwhe-95pM"},"source":["# 2. Configuration acording to our dataset\r\n","\r\n","First, we will import a few libraries. Then we will give the path to the trained weights file. This could be the COCO weights file or your last saved weights file (checkpoint). The log directory is where all our data will be stored when training begins. The model weights at every epoch are saved in .h5 format in the directory so if the training gets hindered due to any reason you can always start from where you left off by specifying the path to the last saved model weights. For instance, if I am training my model for 10 epochs and at epoch 3 my training is obstructed then I will have 3 .h5 files stored in my logs directory. And now I do not have to start my training from the beginning. I can simply change my weights path to the last weights file e.g. ‘mask_rcnn_object_0003.h5’."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wqqgejDfxbb_","executionInfo":{"status":"ok","timestamp":1615192389111,"user_tz":-60,"elapsed":8377,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}},"outputId":"5b1e221c-d02d-47da-b443-02d09ea27ed3"},"source":["# imports\r\n","import os\r\n","import sys\r\n","import json\r\n","import datetime\r\n","import numpy as np\r\n","import skimage.draw\r\n","import cv2\r\n","from mrcnn.visualize import display_instances\r\n","import matplotlib.pyplot as plt\r\n","\r\n","# Root directory of the project\r\n","ROOT_DIR = os.path.abspath(\"/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/\")\r\n","\r\n","# Import Mask RCNN\r\n","sys.path.append(ROOT_DIR)  # To find local version of the library\r\n","from mrcnn.config import Config\r\n","from mrcnn import model as modellib, utils\r\n","\r\n","# Path to trained weights file\r\n","COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\r\n","\r\n","# Directory to save logs and model checkpoints\r\n","DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n","\r\n","class CustomConfig(Config):\r\n","    \"\"\"Configuration for training on the dataset.\r\n","    Derives from the base Config class and overrides some values.\r\n","    \"\"\"\r\n","    # Give the configuration a recognizable name\r\n","    NAME = \"object\"\r\n","    \r\n","    # We use a GPU with 12GB memory, which can fit two images.\r\n","    # Adjust down if you use a smaller GPU.\r\n","    IMAGES_PER_GPU = 2\r\n","\r\n","    # Number of classes (including background)\r\n","    NUM_CLASSES = 1 + 1  # Background + Fire\r\n","\r\n","    # Number of training steps per epoch\r\n","    STEPS_PER_EPOCH = 100\r\n","\r\n","    # Skip detections with < 90% confidence\r\n","    DETECTION_MIN_CONFIDENCE = 0.9"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAFJiMfjyN8v","executionInfo":{"status":"ok","timestamp":1615192389113,"user_tz":-60,"elapsed":8358,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}},"outputId":"6fb28951-c5dd-45f2-cb76-6fe295c1e0de"},"source":["# Display CPU information\r\n","!cat /proc/cpuinfo"],"execution_count":5,"outputs":[{"output_type":"stream","text":["processor\t: 0\n","vendor_id\t: AuthenticAMD\n","cpu family\t: 23\n","model\t\t: 49\n","model name\t: AMD EPYC 7B12\n","stepping\t: 0\n","microcode\t: 0x1000065\n","cpu MHz\t\t: 2250.000\n","cache size\t: 512 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 0\n","initial apicid\t: 0\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n","bugs\t\t: sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass\n","bogomips\t: 4500.00\n","TLB size\t: 3072 4K pages\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 48 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 1\n","vendor_id\t: AuthenticAMD\n","cpu family\t: 23\n","model\t\t: 49\n","model name\t: AMD EPYC 7B12\n","stepping\t: 0\n","microcode\t: 0x1000065\n","cpu MHz\t\t: 2250.000\n","cache size\t: 512 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 1\n","initial apicid\t: 1\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n","bugs\t\t: sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass\n","bogomips\t: 4500.00\n","TLB size\t: 3072 4K pages\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 48 bits physical, 48 bits virtual\n","power management:\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NLet-5TP-iLb"},"source":["# 3. Setting up the CustomDataset class\r\n","\r\n","The class below contains 3 crucial methods for our custom dataset. This class inherits from “utils.Dataset” which we imported in the 1st step. The ‘load_custom’ method is for saving the annotations along with the image. Here we extract polygons and the respective classes.\r\n","\r\n","```\r\n","polygons = [r[‘shape_attributes’] for r in a[‘regions’]]\r\n","\r\n","objects = [s[‘region_attributes’][‘name’] for s in a[‘regions’]]\r\n","```\r\n","\r\n","Polygons variable contains the coordinates of the masks. Objects variable contains the names of the respective classes.\r\n","The ‘load_mask’ method loads the masks as per the coordinates of polygons. The mask of an image is nothing but a list containing binary values. Skimage.draw.polygon() does the task for us and returns the indices for the coordinates of the mask.\r\n"]},{"cell_type":"code","metadata":{"id":"RVQS-LGNyWqu","executionInfo":{"status":"ok","timestamp":1615192391993,"user_tz":-60,"elapsed":778,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}}},"source":["class CustomDataset(utils.Dataset):\r\n","\r\n","    def load_custom(self, dataset_dir, subset):\r\n","        \"\"\"Load a subset of the Fire-NonFire dataset.\r\n","        dataset_dir: Root directory of the dataset.\r\n","        subset: Subset to load: train or validation\r\n","        \"\"\"\r\n","        # Add classes. We have only one class to add.\r\n","        self.add_class(\"object\", 1, \"fire\")\r\n","        # self.add_class(\"object\", 2, \"xyz\") #likewise\r\n","\r\n","        # Train or validation dataset?\r\n","        assert subset in [\"train\", \"validation\"]\r\n","        dataset_dir = os.path.join(dataset_dir, subset)\r\n","\r\n","        # Load annotations\r\n","        # VGG Image Annotator saves each image in the form:\r\n","        # { 'filename': '28503151_5b5b7ec140_b.jpg',\r\n","        #   'regions': {\r\n","        #       '0': {\r\n","        #           'region_attributes': {},\r\n","        #           'shape_attributes': {\r\n","        #               'all_points_x': [...],\r\n","        #               'all_points_y': [...],\r\n","        #               'name': 'polygon'}},\r\n","        #       ... more regions ...\r\n","        #   },\r\n","        #   'size': 100202\r\n","        # }\r\n","        # We mostly care about the x and y coordinates of each region\r\n","        annotations1 = json.load(open(os.path.join(dataset_dir, \"via_labels.json\")))\r\n","        #print(annotations1)\r\n","        annotations = list(annotations1.values())  # don't need the dict keys\r\n","\r\n","        # The VIA tool saves images in the JSON even if they don't have any\r\n","        # annotations. Skip unannotated images.\r\n","        annotations = [a for a in annotations if a['regions']]\r\n","        \r\n","        # Add images\r\n","        for a in annotations:\r\n","            # print(a)\r\n","            # Get the x, y coordinates of points of the polygons that make up\r\n","            # the outline of each object instance. There are stores in the\r\n","            # shape_attributes (see json format above)\r\n","            polygons = [r['shape_attributes'] for r in a['regions']] \r\n","            objects = [s['region_attributes']['class'] for s in a['regions']]\r\n","            print(\"objects:\",objects)\r\n","            name_dict = {\"fire\": 1} #,\"xyz\": 3}\r\n","            # key = tuple(name_dict)\r\n","            num_ids = [name_dict[a] for a in objects]\r\n","     \r\n","            # num_ids = [int(n['Event']) for n in objects]\r\n","            # load_mask() needs the image size to convert polygons to masks.\r\n","            # Unfortunately, VIA doesn't include it in JSON, so we must read\r\n","            # the image. This is only managable since the dataset is tiny.\r\n","            print(\"numids\",num_ids)\r\n","            image_path = os.path.join(dataset_dir, a['filename'])\r\n","            image = skimage.io.imread(image_path)\r\n","            height, width = image.shape[:2]\r\n","\r\n","            self.add_image(\r\n","                \"object\",  ## for a single class just add the name here\r\n","                image_id=a['filename'],  # use file name as a unique image id\r\n","                path=image_path,\r\n","                width=width, height=height,\r\n","                polygons=polygons,\r\n","                num_ids=num_ids\r\n","                )\r\n","\r\n","    def load_mask(self, image_id):\r\n","        \"\"\"Generate instance masks for an image.\r\n","       Returns:\r\n","        masks: A bool array of shape [height, width, instance count] with\r\n","            one mask per instance.\r\n","        class_ids: a 1D array of class IDs of the instance masks.\r\n","        \"\"\"\r\n","        # If not a Horse/Man dataset image, delegate to parent class.\r\n","        image_info = self.image_info[image_id]\r\n","        if image_info[\"source\"] != \"object\":\r\n","            return super(self.__class__, self).load_mask(image_id)\r\n","\r\n","        # Convert polygons to a bitmap mask of shape\r\n","        # [height, width, instance_count]\r\n","        info = self.image_info[image_id]\r\n","        if info[\"source\"] != \"object\":\r\n","            return super(self.__class__, self).load_mask(image_id)\r\n","        num_ids = info['num_ids']\r\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\r\n","                        dtype=np.uint8)\r\n","        for i, p in enumerate(info[\"polygons\"]):\r\n","            # Get indexes of pixels inside the polygon and set them to 1\r\n","        \trr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\r\n","\r\n","        \tmask[rr, cc, i] = 1\r\n","\r\n","        # Return mask, and array of class IDs of each instance. Since we have\r\n","        # one class ID only, we return an array of 1s\r\n","        # Map class names to class IDs.\r\n","        num_ids = np.array(num_ids, dtype=np.int32)\r\n","        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\r\n","\r\n","    def image_reference(self, image_id):\r\n","        \"\"\"Return the path of the image.\"\"\"\r\n","        info = self.image_info[image_id]\r\n","        if info[\"source\"] == \"object\":\r\n","            return info[\"path\"]\r\n","        else:\r\n","            super(self.__class__, self).image_reference(image_id)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71ecCFwYDAV0"},"source":["# 4. Create the training model\r\n","\r\n","First, we will create an instance of CustomDataset class for the training dataset. Similarly, create another instance for the validation dataset. Then we will call the load_custom() method by passing in the name of the directory where our data is stored. ‘layers’ parameter is set to ‘heads’ here as I am not planning to train all the layers in the model. This will only train some of the top layers in the architecture. If you want you can set ‘layers’ to ‘all’ for training all the layers in the model.\r\n","I am running the model for only 10 epochs only as this tutorial is supposed to just guide you."]},{"cell_type":"code","metadata":{"id":"DrMYNOtGDW5H","executionInfo":{"status":"ok","timestamp":1615192394952,"user_tz":-60,"elapsed":652,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}}},"source":["def train(model):\r\n","    \"\"\"Train the model.\"\"\"\r\n","    # Training dataset.\r\n","    dataset_train = CustomDataset()\r\n","    dataset_train.load_custom(\"/content/dataset\", \"train\")\r\n","    dataset_train.prepare()\r\n","\r\n","    # Validation dataset\r\n","    dataset_val = CustomDataset()\r\n","    dataset_val.load_custom(\"/content/dataset\", \"validation\")\r\n","    dataset_val.prepare()\r\n","\r\n","    # *** This training schedule is an example. Update to your needs ***\r\n","    # Since we're using a very small dataset, and starting from\r\n","    # COCO trained weights, we don't need to train too long. Also,\r\n","    # no need to train all layers, just the heads should do it.\r\n","    print(\"Training network heads\")\r\n","    model.train(dataset_train, dataset_val,\r\n","                learning_rate=config.LEARNING_RATE,\r\n","                epochs=10,\r\n","                layers='heads')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISHaMqFED3je"},"source":["# 5. Setup before the training\r\n","\r\n","This step is for setting up the model for training and downloading and loading pre-trained weights. You can load the weights of COCO or your last saved model.\r\n","The call ‘modellib.MaskRCNN()’ is the step where you can get lots of errors if you have not chosen the right versions in the 1st section. This method has a parameter ‘mode’ which decides whether we want to train the model or test the model. If you want to test set ‘mode’ to ‘inference’. ‘model_dir’ is for saving the data while training for backup. Then, we download the pre-trained COCO weights in the next step.\r\n","\r\n","**Note:** *If you want to resume training from a saved point then you need to change ‘weights_path’ to the path where your .h5 file is stored.*\r\n"]},{"cell_type":"code","metadata":{"id":"KZ_Q5E6p0xQw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615192416628,"user_tz":-60,"elapsed":19615,"user":{"displayName":"Lucas Herranz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gif9nvvPAzk_dvvjIEH6hHiBjc7wkiYzeLtKEUduQ=s64","userId":"17358744960044814692"}},"outputId":"72ca284a-8865-42fc-b9d1-2c2a882d00a4"},"source":["config = CustomConfig()\r\n","model = modellib.MaskRCNN(mode=\"training\", config=config,\r\n","                                  model_dir=DEFAULT_LOGS_DIR)\r\n","\r\n","weights_path = COCO_WEIGHTS_PATH\r\n","        # Download weights file\r\n","if not os.path.exists(weights_path):\r\n","  utils.download_trained_weights(weights_path)\r\n","\r\n","model.load_weights(weights_path, by_name=True, exclude=[\r\n","            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\r\n","            \"mrcnn_bbox\", \"mrcnn_mask\"])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n","\n","WARNING:tensorflow:From /content/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","Downloading pretrained model to /content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/mask_rcnn_coco.h5 ...\n","... done downloading pretrained model!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AZAF4FXsExIZ"},"source":["# 6. Start training\r\n","\r\n","This step should not throw any error if you have followed the steps above and training should start smoothly. Remember we need to have Keras version 2.2.5 for this step to run error-free."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49OJ3kyJFBnS","outputId":"e536ff30-2fef-416f-ae0d-3d7fb9d89ee1"},"source":["train(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["objects: ['fire', 'fire', 'fire']\n","numids [1, 1, 1]\n","objects: ['fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire', 'fire', 'fire']\n","numids [1, 1, 1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire', 'fire', 'fire']\n","numids [1, 1, 1]\n","objects: ['fire', 'fire', 'fire']\n","numids [1, 1, 1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1, 1]\n","objects: ['fire', 'fire', 'fire']\n","numids [1, 1, 1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","objects: ['fire', 'fire', 'fire']\n","numids [1, 1, 1]\n","objects: ['fire', 'fire', 'fire', 'fire', 'fire', 'fire']\n","numids [1, 1, 1, 1, 1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","objects: ['fire']\n","numids [1]\n","objects: ['fire', 'fire']\n","numids [1, 1]\n","Training network heads\n","\n","Starting at epoch 0. LR=0.001\n","\n","Checkpoint Path: /content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs/object20210308T0833/mask_rcnn_object_{epoch:04d}.h5\n","Selecting layers to train\n","fpn_c5p5               (Conv2D)\n","fpn_c4p4               (Conv2D)\n","fpn_c3p3               (Conv2D)\n","fpn_c2p2               (Conv2D)\n","fpn_p5                 (Conv2D)\n","fpn_p2                 (Conv2D)\n","fpn_p3                 (Conv2D)\n","fpn_p4                 (Conv2D)\n","In model:  rpn_model\n","    rpn_conv_shared        (Conv2D)\n","    rpn_class_raw          (Conv2D)\n","    rpn_bbox_pred          (Conv2D)\n","mrcnn_mask_conv1       (TimeDistributed)\n","mrcnn_mask_bn1         (TimeDistributed)\n","mrcnn_mask_conv2       (TimeDistributed)\n","mrcnn_mask_bn2         (TimeDistributed)\n","mrcnn_class_conv1      (TimeDistributed)\n","mrcnn_class_bn1        (TimeDistributed)\n","mrcnn_mask_conv3       (TimeDistributed)\n","mrcnn_mask_bn3         (TimeDistributed)\n","mrcnn_class_conv2      (TimeDistributed)\n","mrcnn_class_bn2        (TimeDistributed)\n","mrcnn_mask_conv4       (TimeDistributed)\n","mrcnn_mask_bn4         (TimeDistributed)\n","mrcnn_bbox_fc          (TimeDistributed)\n","mrcnn_mask_deconv      (TimeDistributed)\n","mrcnn_class_logits     (TimeDistributed)\n","mrcnn_mask             (TimeDistributed)\n"],"name":"stdout"},{"output_type":"stream","text":["/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n"," 99/100 [============================>.] - ETA: 1:55 - loss: 2.1364 - rpn_class_loss: 0.1195 - rpn_bbox_loss: 0.6244 - mrcnn_class_loss: 0.0915 - mrcnn_bbox_loss: 0.6789 - mrcnn_mask_loss: 0.6220"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MKTSNhQORvWz"},"source":["import shutil\r\n","shutil.copytree('/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs', '/content/drive/MyDrive/mrcnn_fire/logs')"],"execution_count":null,"outputs":[]}]}